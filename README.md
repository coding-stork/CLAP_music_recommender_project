# CLAP Recommender Project
_Solo University Project_

![project_idea](/md9.png)

## IDEA

The idea behind this project was to lay the foundations for **a music recommendation system capable to consider the tracks' audio features** (spectral, tonal, musical features) during the recommendation process and not just the user-item or item-item relations codified by graphs or metadata. To do this, an experimental model was devised by combining the approach behind self-supervised contrastive learning models such as OpenAI's **CLIP** with the most cutting-edge models in **audio processing** and **heterogeneous graph / knowledge graph processing**. The goal was to leverage the cross-modality learning power of contrastive learning for a unique content-based recommender system, an approach that appears quite unexplored, and to test it on music recommendation. Since graph-based music recommender systems tend to favour less-relevant but highly-connected nodes (that is, popular tracks) over more-relevant but less-connected (or "obscure") ones, the addition of audio features conditioning and the ability to relate audio features to graph topology on a shared latent space could help tackling this problem, which in turn would benefit smaller artists, as their relevant tracks would no longer be buried under more popular ones during a recommendation task. Also, the need of a music recommendation system more focused on recommending tracks based on similar observable stylistic features instead of music enjoyed by users with similar taste is quite unexplored, and it is an interesting exploration which could also act as an information retrieval system under certain conditions (for example in finding temp tracks for visual projects and movies, or checking for involuntary plagiarism of a song or style).

## MODELS USED

The models utilized as bases for this project are the following:

[**CLAP**](https://github.com/LAION-AI/CLAP): a cross-modal contrastive learning model from LaionAI that uses a spectrogram transformer (**HTS-AT**) and a LLM (**RoBERTa**) as encoders to learn the shared latent representation of audio data and text descriptions, inspired by OpenAI's **CLIP** model.

[**HINormer**](https://github.com/Ffffffffire/HINormer): a blend of a transformer model and a **HIN (Heterogeneous Information Network)** that works on Heterogeneous Graphs and can capture both local structural features and heterogeneous relations between nodes, generating rich information vectors. It's mainly used for node classification but it can be used as an encoder too.

[**Relphormer**](https://github.com/zjunlp/Relphormer): a cutting-edge blend of a **Large Language Model** (it's based on BERT) and a **Graph Transformer** that works well on Knowledge Graphs, capturing both the semantic and syntactic properties of natural language and also the structural properties of the graph. Its similarities with BERT-based models and its ability to understand graphs more deeply than just through structural features make it a great candidate for this project.

<p align="center">
<img src="https://raw.githubusercontent.com/LAION-AI/CLAP/main/assets/audioclip-arch.png" alt="CLAP architecture" width="60%"/>
<p align="center"><i>CLAP's original architecture</i></p>
</p>

## APPROACH
The approach used for the development of the core idea was very simple: swap the text encoder inside CLAP with a graph encoder, train CLAP on a dataset comprised of audio tracks and a large graph modeling the musical relationships between songs, then use the embeddings learned through positive association during contrastive learning for item-to-item recommendation tasks measuring the cosine similarity between vectors in different latent spaces. So the final model is a contrastive learning model with a spectrogram transformer and a graph transformers as encoders, and the training pipeline associates each node representing a track in the graph with the audio files of the track itself, learning the relationship between the features of the two different media domains and using it to create strong embedding vectors for the recommendation task.

## DEVELOPMENT

This project required a large and custom **dataset**, containing both a graph modeling musical relationships between songs, playlists, artists, genres etc..., and the audio data of the songs themselves. Since there were no existing datasets of this nature, a custom one was manually assembled. Using the data contained in the **[6K Spotify Playlists](https://www.kaggle.com/datasets/viktoriiashkurenko/278k-spotify-songs)** dataset, a heterogeneous graph containing 20k randomly selected songs was created. This graph contained track nodes, artist nodes, playlist nodes, genre nodes and descriptor nodes (obtained from [a different dataset](https://www.kaggle.com/datasets/tobennao/rym-top-5000)), along with all the edges representing relationships between these musical entities. The final graph ended up having 45.308 nodes and 207.650 edges. Then the 20000 tracks were slowly downloaded from youtube. Since HTS-AT, the spectrogram encoder, works best with 10-seconds long audio tracks, the downloaded songs were cut into five different 10-seconds-long samples. This fails to capture the whole musical discourse or arrangement of various tracks, but due to uncertainty about the project results, lack of resources and the difficulty of working with contrastive learning models, the spectrogram encoder was not exchanged for a more fitting model during the initial development. Still, this technique captures the tonal and musical features of a track well enough, recognizing musical instruments and various stylistic elements. During training, the five audios are associated with a single embedding vector representing the track node, during five different training steps, to ensure generalization and capture the audio features more broadly. 8000 tracks were used as the training set while 12000 tracks were used as the validation set, to have a large pool of tracks for the testing phase. The model was trained on Colab and plenty of different training session took place during the ablation process to try and improve the model's accuracy.

To create the model, **HINormer** was initially chosen as the Graph Encoder due to the balance between simplicity and effectiveness. To use it as an encoder, its last layer (the prediction layer) was removed and the output tensor associated with the first node in the input sequence was taken as the feature vector (HINormer doesn't work with single nodes, but with sequences of nodes sampled starting from a base node, which is the first in the sequence). The performance of CLAP with HINormer as a backbone, however, was not ideal, and testing the model revealed some issues, such as an imbalance in weight adjustment between the encoders and poor results in recommendation tests. Some changes were implemented to handle this, such as adding back the feed-forward layer (removed by HINormer's authors), adding an embedding layer and creating a whole self-supervised contrastive learning pipeline with graph augmentation on HINormer to generate strong starting embedding for the nodes. Some of these changes improved the results, but they were still not good enough. For this reason, **Relphormer** was chosen as the Graph Encoder instead, thanks to its similarity to RoBERTa's architecture, and the heterogeneous graph used as the dataset was turned into a **Knowledge Graph**. It was also enriched with more information of music genres and playlist descriptions to increase the knowledge of the model on the general musical landscape. CLAP+Relphormer demonstrates better performance and better results, although it's not immune from false positives. As the last stage of the development, a custom algorithm was written to use five different **zero-shot classification** tasks on the embeddings generated both by CLAP+HINormer and CLAP+Relphormer for the purpose of rebalancing the results across different models, filtering out the outliers and giving more weight to tracks recommended by more than one model. This is not a final product in any way and it was just a way to remove outliers and attempt to polish the model a bit before the testing phase.

## RESULTS

The results can be considered mostly positive, given the highly experimental nature of the project and the lack of resources as a solo project, although it's difficult to compare this recommender system to existing infrastructure due to how differently it works and the uniqueness of its focus on more tonally-based recommendations. The metrics for CLAP+Relphormer on the zero-shot classification task (which involves taking the embedding vector outputted by an encoder, comparing it to all of the vectors in the validation set outputted by the other encoder, ranking them by cosine similarity, and looking at where the vector corresponding to the correct track is placed in the similarity ranking), shows a mean rank of roughly 203/207 out of 12000 and a median rank of 54, while the R@10 (the percentage of recommendations which had the correct track in the top 10 of the ranking) is 0.2, so 20%.

Since these metrics are not entirely appropriate to judge the quality of such a system, a lot of emphasys was also put on having expert user test the model. The model was tested by 7 users, all highly familiar with music recommendation systems and their purpose, as they were asked to provide some music tracks that the model would use for a cold-start recommendation starting from a single song. They were then asked to give a score on the precision of the recommendation and on personal taste for each recommended track. The average of these votes was 2.86 out of 5 for precision and 5.67 out of 10 for personal taste. Thus, there is a lot of room for improvement. However, the testers were also asked to provide feedback on recommendations that seemed particularly in line with the intention behind the idea of the model, that is, a recommendation that gives more attention to style and tonality than to shared musical landscape or popularity. The results this time were positive, as a many cases emerged where the model visibly behaved in a way that consolidates the expression of this unique approach, such as recommending reggae tracks or rnb-inspired reggae tracks during a recommendation based on a track by Amy Winehouse with reggae influences in rhythm and sound but without the explicit presence of reggae in its metadata (the node representing Amy Winehouse is quite far from the reggae cluster), the recommendation of a bunch of EDM tracks with a chopped-up female voice sample and a psytrance bass in the drop based on a track with these notable characteristics (again, these are not explicitly stated in the metadata, as this is not a consolidated genre but merely a common style), and more such cases. The variation in popularity among the recommended tracks is also greater, as both popular and very obscure tracks are recommended in the top spots of the ranking, so it seems that the model is headed towards the direction of limiting the popularity inflation of collaborative filtering-based recommender system, although this cannot be demonstrated with such a small dataset (12000 nodes) compared to the enormous graphs used by platforms like Spotify or Apple Music, where the imbalance in popularity between top and bottom tracks is astronomical by comparison. There are plenty of ways in which this project can be improved, such as a better dataset and the use of a proper autoregressive model capable of extracting a feature vector from the entire song as the spectrogram transformer, and I will update this page if I manage to get better results.

